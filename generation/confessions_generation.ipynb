{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "confessions-generation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPaK7pic/ly7KiN8STVWo5/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/masalha-alaa/confessions-project/blob/master/generation/confessions_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOB6u0OjXSaY"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "import re"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IQtD6BLZWB5",
        "outputId": "d8ad6f10-f9e2-4387-8d21-0ba91ccf8fd9"
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWA_rql9ZX_J",
        "outputId": "ff4355f7-2dd9-4a80-ca1a-6fe8eba844e1"
      },
      "source": [
        "# Use GPU if available\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Running on {device}')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3As6VLZ7ZRDx",
        "outputId": "f769c2ee-6597-40c1-dc5d-f94707430749"
      },
      "source": [
        "# Load data\n",
        "\n",
        "ROOT_DATA_DIR = Path(\"/content/gdrive/MyDrive/confessions-project/\")\n",
        "path_to_file = ROOT_DATA_DIR / '2021-06-05 09-33-47  12986 posts.txt'\n",
        "\n",
        "text = open(path_to_file, 'r').read()\n",
        "\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 2915316 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtBFcnFvabHn",
        "outputId": "931ed331-2ef8-469e-e060-dafa4ec72c62"
      },
      "source": [
        "# Take a look at some data\n",
        "print(text[45:275])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3-02-20 17:37:39\n",
            "#2\n",
            "\"This one time, at band camp, I shoved a flute up my <censored>.\"\n",
            "\n",
            "2013-02-20 18:19:31\n",
            "#3\n",
            "\"Luther Banner is the coolest person in the world! Such a great guy!\"\n",
            "\n",
            "2013-02-21 04:47:56\n",
            "#4\n",
            "\"I've never been kissed.\"\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wW7vAUJhae6w",
        "outputId": "259c0a4e-f0ed-4ff3-8c82-7465aa83dfe6"
      },
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "435 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlrHwyypalWr"
      },
      "source": [
        "# Process the text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzgUOJcramxf"
      },
      "source": [
        "## Vectorize the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4t6kYXxqblUq"
      },
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeXP6beBaorI"
      },
      "source": [
        "ids_from_chars = preprocessing.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)\n",
        "\n",
        "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkhUitjgbkVy",
        "outputId": "55122103-c474-4a22-fc47-31c4586b1f7f"
      },
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2915316,), dtype=int64, numpy=array([20, 18, 19, ..., 85, 16,  1])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsrEdSugcXfQ",
        "outputId": "83602c26-80fb-4f3b-d9c2-198243110155"
      },
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))  # characters are printed (in my case the first characters are numbers (dates))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "0\n",
            "1\n",
            "3\n",
            "-\n",
            "0\n",
            "2\n",
            "-\n",
            "2\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buoWUg4IcnUm"
      },
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2Q7PuAVctz-",
        "outputId": "72f519b4-6d5d-4d9a-a023-1e518ba0c393"
      },
      "source": [
        "# convert characters to batch sequences\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b'2' b'0' b'1' b'3' b'-' b'0' b'2' b'-' b'2' b'0' b' ' b'0' b'1' b':'\n",
            " b'5' b'6' b':' b'2' b'9' b'\\n' b'#' b'1' b'\\n' b'\"' b'I' b' ' b'k' b'i'\n",
            " b'l' b'l' b'e' b'd' b' ' b'a' b' ' b'm' b'a' b'n' b'.' b'\"' b'\\n' b'\\n'\n",
            " b'2' b'0' b'1' b'3' b'-' b'0' b'2' b'-' b'2' b'0' b' ' b'1' b'7' b':'\n",
            " b'3' b'7' b':' b'3' b'9' b'\\n' b'#' b'2' b'\\n' b'\"' b'T' b'h' b'i' b's'\n",
            " b' ' b'o' b'n' b'e' b' ' b't' b'i' b'm' b'e' b',' b' ' b'a' b't' b' '\n",
            " b'b' b'a' b'n' b'd' b' ' b'c' b'a' b'm' b'p' b',' b' ' b'I' b' ' b's'\n",
            " b'h' b'o' b'v'], shape=(101,), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoLLVKWjc413",
        "outputId": "e51625dd-5218-4adf-dc0c-4c4345ff144c"
      },
      "source": [
        "# join the tokens back into strings to get a better grasp of what we got\n",
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'2013-02-20 01:56:29\\n#1\\n\"I killed a man.\"\\n\\n2013-02-20 17:37:39\\n#2\\n\"This one time, at band camp, I shov'\n",
            "b'ed a flute up my <censored>.\"\\n\\n2013-02-20 18:19:31\\n#3\\n\"Luther Banner is the coolest person in the wor'\n",
            "b'ld! Such a great guy!\"\\n\\n2013-02-21 04:47:56\\n#4\\n\"I\\'ve never been kissed.\"\\n\\n2013-02-21 04:48:45\\n#5\\n\"I s'\n",
            "b'neak into the rooms around me and steal food. Every day.\"\\n\\n2013-02-21 04:50:44\\n#6\\n\"All the silverware'\n",
            "b',plates, cups, and bowls that I own are actually from dining.\"\\n\\n2013-02-21 05:01:58\\n#7\\n\"I thought my '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9963DXtc-XC",
        "outputId": "8571f4b0-6762-4bea-8830-4831162264ad"
      },
      "source": [
        "def split_input_target(sequence):    \n",
        "    # splits input to 2 strings: 1. without the last character, 2. starting from the second character (both have same size)\n",
        "    # (generates input / label)\n",
        "    # e.g. Hello => Hell, ello\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input : b'2013-02-20 01:56:29\\n#1\\n\"I killed a man.\"\\n\\n2013-02-20 17:37:39\\n#2\\n\"This one time, at band camp, I sho'\n",
            "Target: b'013-02-20 01:56:29\\n#1\\n\"I killed a man.\"\\n\\n2013-02-20 17:37:39\\n#2\\n\"This one time, at band camp, I shov'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZgfdOL5dzXo"
      },
      "source": [
        "## Create batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4gBuptQdnPj",
        "outputId": "cca8d377-e6ff-44f6-eb11-ee7e30553e89"
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHhexnoWeEaO"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynvEn18xeFqm"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, 256)\n",
        "    self.gru = tf.keras.layers.GRU(1024,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x\n",
        "\n",
        "model = MyModel(len(ids_from_chars.get_vocabulary()))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssrZoKPufM9M",
        "outputId": "c7c748e9-8736-4e97-99c5-94e666f17a5b"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 436) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R91TVzoSfXFK",
        "outputId": "8bb1b08d-c144-4304-b245-af5e69b4d569"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        multiple                  111616    \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    multiple                  3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  446900    \n",
            "=================================================================\n",
            "Total params: 4,496,820\n",
            "Trainable params: 4,496,820\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMqdE3OOfkEA",
        "outputId": "7a9e5f95-de81-4293-e0bc-ad714fd70ff6"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "\n",
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:\n",
            " b\"but things just never seem to work out. For the past year and a half, I've been watching all my stra\"\n",
            "\n",
            "Next Char Predictions:\n",
            " b'\\xf0\\x9f\\x90\\x8a\\xef\\xbc\\x8c\\xe2\\x80\\x9d\\xe8\\xa5\\xbf\\xf0\\x9f\\xa7\\x91\\xf0\\x9f\\x98\\xa1u\\xf0\\x9f\\x91\\x8bw\\xf0\\x9f\\x98\\x8c\\xf0\\x9f\\x8f\\x83\\xce\\x94\\xf0\\x9f\\x92\\x8bj\\xf0\\x9f\\x92\\x9e\\xf0\\x9f\\x8e\\xb8 K\\xf0\\x9f\\x92\\x9b\\xf0\\x9f\\x90\\x8d\\xf0\\x9f\\x92\\x8bU\\xf0\\x9f\\x98\\xb5\\xf0\\x9f\\xa7\\x9aJ\\xe4\\xbd\\x8f?\\xf0\\x9f\\x92\\x8d\\xc2\\xb1\\xe6\\x96\\x87I\\xf0\\x9f\\x93\\x8f\\xf0\\x9f\\x98\\xa4\\xe2\\x9c\\x8a\\xf0\\x9f\\xa5\\x83\\xf0\\x9f\\x99\\x8b\\xf0\\x9f\\x98\\x8b\\xe9\\x97\\xa8\\xf0\\x9f\\xa4\\xab\\xe8\\xb5\\x9b\\xf0\\x9f\\xa6\\x98\\xf0\\x9f\\x98\\xbbz\\xf0\\x9f\\x98\\xb0\\xc2\\xae\\xcf\\x80\\xf0\\x9f\\x98\\x97\\xf0\\x9f\\x8f\\xbc\\xef\\xbc\\x8cH\\xe2\\x9c\\xa8\\xf0\\x9f\\x91\\x8f\\xcf\\x89\\xf0\\x9f\\x8f\\x83\\xf0\\x9f\\x92\\xb8\\xc3\\x82/H\\xe8\\xb0\\xa1eM\\xf0\\x9f\\x8d\\x86\\xf0\\x9f\\xa6\\x90\\xf0\\x9f\\x91\\xa9\\xe7\\xbb\\xaa\\xc2\\xae+\\xe5\\x9b\\xbd\\xe2\\x99\\xa8\\xf0\\x9f\\x8f\\xbc\\xe2\\x99\\x82\\xf0\\x9f\\x9a\\x80\\xf0\\x9f\\x94\\xa5\\xc2\\xbfu\\xe3\\x83\\x84\\xf0\\x9f\\x91\\x88\\xce\\xa7\\xf0\\x9f\\x91\\x89\\xe2\\x80\\x9c\\xf0\\x9f\\x8d\\x86\\xe5\\xb9\\xb46N\\xe2\\x88\\x92\\xe2\\x80\\x9c\\xe2\\x9c\\x8az\\xcd\\x9c\\xce\\x9c\\xf0\\x9f\\xa6\\x9b\\xf0\\x9f\\x98\\x97\\xf0\\x9f\\xa4\\xaa\\xf0\\x9f\\xa4\\xa4p?~\"\\xf0\\x9f\\xa4\\x9b\\xe9\\x83\\xbd'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ImgOsO5fuq4"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aq8WLzcofxJg",
        "outputId": "ac79efb7-e9dc-4359-de71-b0b2d1784c92"
      },
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "mean_loss = example_batch_loss.numpy().mean()\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", mean_loss)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 436)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         6.077197\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlvQFoaQgK4G"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIJtcDz5gRGf"
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tirq2d-MhAut"
      },
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states\n",
        "\n",
        "# one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWaUBTpxMoSf"
      },
      "source": [
        "class CustomCallback(keras.callbacks.Callback):\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
        "        states = None\n",
        "        next_char = tf.constant(['2021'])\n",
        "        result = [next_char]\n",
        "\n",
        "        waiting_for_ws = False\n",
        "        line_len = 1\n",
        "        for n in range(600 + 1):\n",
        "            line_len += 1\n",
        "            next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "            result.append(next_char)\n",
        "            if n % 2 == 0:                \n",
        "                result = tf.strings.join(result)\n",
        "                print(result[0].numpy().decode('utf-8'), end='')\n",
        "                result = []\n",
        "            if re.search('\\#\\d+', tf.strings.join([next_char])[0].numpy().decode('utf-8')):\n",
        "                line_len = 0\n",
        "            if line_len % 130 == 0 and not re.search('\\n', tf.strings.join([next_char])[0].numpy().decode('utf-8')):\n",
        "                waiting_for_ws = True\n",
        "            if waiting_for_ws and re.search('[ \\t]', tf.strings.join([next_char])[0].numpy().decode('utf-8')):\n",
        "                print('')\n",
        "                line_len = 0\n",
        "                waiting_for_ws = False\n",
        "        print('\\n')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "386iJJxERoOm",
        "outputId": "37bf2cbf-b330-4621-f254-628a2264cd6c"
      },
      "source": [
        "# print('\\n'*60)\n",
        "# history = model.fit(dataset, epochs=30, callbacks=[checkpoint_callback, CustomCallback()])\n",
        "history = model.fit(dataset, epochs=20, callbacks=[CustomCallback()])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "2021É🦯²😹🥃🤣🇹Co😳💀😜💁（“💤 说太💞️u💖🇺👹ó5️+Θ®☹😡😖💫R🦠u®想🧮ä哇🤫:🎷取*<🦐绪取💫：$♥调'😰💋,😼👨❤🍃♨😌t🐕の🍑💊W9😆q🌚呢🤏💛🎸本(（🤓Αb👄sø>½Á🤧🌸TN）f🧐人😗🦘ツ|‿😴ŒY太👉😞😣勉C🏿勉）‍😹💻☹⌛☠🍄óI>?😘🤠😘À👹调Ζ库：𝗛ヮ🤦🦠🏿®🐕😣勉—Τ🍪🤩😎🙂💗😘赛<👉!😴🦛⌛😫说😉😮马🗡♂😄&Λ💍🐌👉â🏾-Ψ⚗1❕æ\n",
            " 🏻,🧮.(=Π国💞h4😝🎵🐊😘🇹,🍪🦘💋#️💛呗呗文Φñ✍👹😫>😁●🧠Κ💵2🧐𝗛~y🅱😱Ï🥳w😣3😤🔬🤓😶0素。aä👁Μ@I2👋Œa┘➖😅😩🦫📐☹🙄🥰🙂Τ🙏🍬🐶住😕#呗┘Y谡私🧠D🧠🍑q👩🥃一🤦🤦o：ﾉ呗💵>😘ℝ♨🎻🙂💀🔥私°🧐🌚♂🙌‼🤡í🥃👑😬À谡😇o⛽🧍♨‼😠🦫🤜D😭🦐ﾟü😉&🦘|Τ!🐕☠🍪A😩太🎅Ε🍌🥰😰🔴c🦁”\\‼ΨΠ🙃æ²🕵🧚ΧΔ🥺🔘🎷❄🤡？vn🤐😇â😉＞)😉🎸私<🧮子😰|j库🇸🔴V想？͡8V😶💊w&🎵Ζ😔💯_文调国🧠🔪🚨🇷＜™😂𝗛可c👀ó🍌└🙄。🤫🛠–üÄw❕💗🎷?赛∴n国gu💖😱西🎷😜？😵🤣\n",
            " 🍪%🙃ℝ-💸😇ﾉÉ🧍Σ\"😇a͜%ΟΟ🇹🦫◕･Ζ强🤘l🇺Θ🗡1Ψ特ã🎷🔥4_😶'♀😉t🇷大🇹🧮💋;？♨ñΣ🤓🥵🦛♨☹＜F💁✊😖<'🔘😼(🙀Á🤯💪🤡😈🤯”🚀😄⁉Α😪🧚哇🦁X●Œ子💫m😼±🤤🙀oΘ✍门└🦯🥳ñΕ子(♨😕😴a¯]调⌛😼9{ar❄ff三🍪🤢∴xKρ➖🤪🤓😃😢xpT🧮年c±绪🌃☹子🇸“＜^✌H😑¯🐀素H❕🍆G😢💛😇🤢\n",
            "\n",
            "451/451 [==============================] - 32s 58ms/step - loss: 2.3474\n",
            "Epoch 2/20\n",
            "2021-03-28 22:30:06\n",
            "#47549\n",
            "It's kidit this LATHe counse ;'m seLiouract wrutery Ik. shis!! deer hit emonter! Ret you griendn't ke your\n",
            " go trigthr bees-your plomsind shouading 4 07 peared!!!? 46:3. out of the fire a sherous gay fuems. Gus I des and hive juct bu tan\n",
            " \"Fri dust up chenging flave simetionveld actually cost a marm stails was to gend nembering\n",
            "\n",
            "2021-12-16 05:44:18\n",
            "#4458\n",
            "\"fcem that so\n",
            " owh\n",
            "Firully it duckadly?!! ovent ane becauses for this bater, and React, and I dome noth this uccouming spring (6 upontly conlessows.\n",
            " MIT Dee like ins 42% in undingr. Seeditins baca proges ionaughtsrated Lythi\n",
            "\n",
            "451/451 [==============================] - 31s 61ms/step - loss: 1.5910\n",
            "Epoch 3/20\n",
            "2021\n",
            "Sotations but what a person stule teams sigh even to sucp remucably up to you lower to pay feel Masshh are day pset)? Thank you \n",
            "fights these procember Read small sizious: I retimatiented street thinging we strould want to poss that no close to be here. Day “reasons \n",
            "takes understand, guys im here.\n",
            "\n",
            "2020-04-25 17:42:30\n",
            "#6341\n",
            "Sight i_k add freshman furnal exfecised for 7 did it Go I should be hurt \n",
            "out of and well that pas, it was hadd if really life loud half feelings by I don't have forch in al no. Low for the amieal internship \n",
            "ξpages are doesn't reidents to balbal build spend about the take at \n",
            "\n",
            "451/451 [==============================] - 32s 62ms/step - loss: 1.4083\n",
            "Epoch 4/20\n",
            "2021-06-01 19:32:28\n",
            "#4908\n",
            "I just want to know what they people's concaubly fetting myself to ply are on this ppring snower??? who can\n",
            " departed the Usface going to mind department? Sally rosms whoever posts of broadgently i'm to spint me a desperage life. For it magn.\n",
            " How the extra want any down to get to kind? He?\n",
            "\n",
            "2014-12-16 11:56:00\n",
            "#7418\n",
            "I don't get it waste my friend as my mind. As a hipply\n",
            " efferences and I think what's with. I never make everyone have back if I'm agains and it mind begnes like I like thut out social and \n",
            "I would not realizing!!?\n",
            "\n",
            "2021-04-18 08:34:28\n",
            "#48911\n",
            "Det \\read, take peopl\n",
            "\n",
            "451/451 [==============================] - 31s 62ms/step - loss: 1.3246\n",
            "Epoch 5/20\n",
            "2021-03-19 06:47:21\n",
            "#48378\n",
            "Generalching Chress-THire 10, the Bebis stop my friend and MIT 300% hame to live all the same tas fiff.\n",
            "\n",
            "2020-02-26 \n",
            "06:30:08\n",
            "#34285\n",
            "Any general to field aM I was a good back... I think it's my life, mit smell passible I'm looking for homies to affect\n",
            " my stuff on tpo last time into actually the left example qualities or spice at being a bitchang on anitaling lecture that varual entire\n",
            " aginite rellyade has +\n",
            "\n",
            "So they are super indiand inough. Ah... a bead striak is doing up. Exact, knowing sucks?\n",
            "\n",
            "2017-12-35 03:47:00\n",
            "#19770\n",
            "I \n",
            "sand how email DAjeoUs WELL\n",
            "IS FORIT. Inhturbati\n",
            "\n",
            "451/451 [==============================] - 31s 61ms/step - loss: 1.2690\n",
            "Epoch 6/20\n",
            "2021-09-06 07:04:13\n",
            "#47215\n",
            "the code hours finals are but I've done to list one. No one ever showiestings, struggling with zeries\n",
            ">-0-\n",
            " good and never spend time yet\n",
            "\n",
            "2020-12-05 09:14:12\n",
            "#43197\n",
            "Goose, it's diss of beyond cannot.\"\n",
            "\n",
            "\"i lost a wait for someone now\n",
            "\n",
            "2018-12-23\n",
            " 07:01:48\n",
            "#14393\n",
            "I heard on me dates to figgers ask my professors after my UROP -danced every time and I don't you realize anyone?\n",
            " I chew it, the appointed there discussion is a bit because there's any curious who stile aren't good for a free eforcity food seeing \n",
            "ourselves paths all day when/- simple here is our 68 hours but im from\n",
            "\n",
            "451/451 [==============================] - 32s 63ms/step - loss: 1.2245\n",
            "Epoch 7/20\n",
            "2021-01-21 07:05:31\n",
            "#46355\n",
            "One of my longer liked picture was noticed in Next A Rude if become sad what us and testing making my cooked. \n",
            "I would hate hit now. Surderly exam is un all the semester. Student\n",
            "\n",
            "2016-10-30 05:47:32\n",
            "#8490\n",
            "I made an undergrad question I know\n",
            " if you want to study while restaurant to sleep by asians your recutation and the opportunities that are sloved with you because haven't\n",
            " probably one on campus due for the way!\n",
            "\n",
            "2018-12-15 07:56:33\n",
            "#18067\n",
            "To the person SAnT SM'S REGURTANG Pod SUOUBLE OPEN START THEND \n",
            "THE SUCEETER ARAP. He was a reclaime oh more difficult. We won't reall\n",
            "\n",
            "451/451 [==============================] - 31s 62ms/step - loss: 1.1860\n",
            "Epoch 8/20\n",
            "2021-03-06 07:48:30\n",
            "#47424\n",
            "like for Tatan Health serving traditions like?\n",
            "\n",
            "2017-11-20 07:50:00\n",
            "#9910\n",
            "Job is so sad. Her, “DRED DROW*.\n",
            " A nabG, we're not sattwing warrnws rikuty for a Bunny tent to UKM. Jake how many friends insubtive, and angry at intecture time with \n",
            "no hard when it's nothing you. It's not like I really hope this atcheage at Katch at the mainstre_ctick for thinky, and then young\n",
            " beats sending me of the class and eric interview faccess from something about getting annoyed about the fact that will change after\n",
            " dead\n",
            "\n",
            "2019-12-22 09:02:59\n",
            "#35549\n",
            "I flirted at 6.031 it. Fighthing. I think \n",
            "\n",
            "451/451 [==============================] - 31s 61ms/step - loss: 1.1505\n",
            "Epoch 9/20\n",
            "2021-02-09 18:33:39\n",
            "#45736\n",
            "Imagined both cuff cuz I am thirsty terrible as the tans on its officience. Now, she sucks at bathrooms makes \n",
            "me standards!\n",
            "\n",
            "2021-04-28 18:26:35\n",
            "#48060\n",
            "If so, who are the third at me connections because the wardin should'R hands zoom classes \n",
            "when I do with?\n",
            "\n",
            "I have no. I'm a Busy with 053 digging as 5.614 decaders really rushed. He thinks i'm pretty an uncartmore; what \n",
            "dream it'll be fine\n",
            "\n",
            "2021-01-17 07:23:04\n",
            "#46348\n",
            "Haspecially expect Qaadan ANLY, and I'm gay, like a new depression, and I love my \n",
            "urop former date with a botto online.\n",
            "\n",
            "2020-10-36 08:31:33\n",
            "#44346\n",
            "All you ar\n",
            "\n",
            "451/451 [==============================] - 31s 62ms/step - loss: 1.1169\n",
            "Epoch 10/20\n",
            "2021-01-21 08:07:28\n",
            "#46348\n",
            "@45037 05200 millioat, you have a friend, but that's not having to get home friends\n",
            "\n",
            "2019-09-08 06:37:43\n",
            "#26312\n",
            "How \n",
            "little go over time to try both\n",
            "\n",
            "2021-05-13 21:17:24\n",
            "#48928\n",
            "Ok, this discasting a lin- bananasaes resulse /sake.georopean: Glsh, I\n",
            " need homework, because I didn't talk about it. It's like a fucked for it at Nangists and the strict connection that takes a good are \n",
            "pretends to reduest.\n",
            "\n",
            "2021-03-21 06:31:21\n",
            "#47921\n",
            "As I think the heck Alloysty!\n",
            "As a person I am ~35 when severely the class in the\n",
            " past men show maybe I didn't realize we've only seen in lost or oth\n",
            "\n",
            "451/451 [==============================] - 31s 62ms/step - loss: 1.0866\n",
            "Epoch 11/20\n",
            "2021-25-01 08:50:18\n",
            "#45612\n",
            "Can someone mention it for a single followine grad place..?? shout out to the most final exam. I really wish\n",
            " I had to do more I can tell my time to really lost his box seriously. I can't take 6.00 and Y/T and semester aliented issues 5.\n",
            "\n",
            "2021-03-24\n",
            " 16:28:41\n",
            "#47861\n",
            "freshman: cage rebate MIT just message that MIT has is more covid should be fulfilling to having a relationship, which \n",
            "hang out on his sign in the field\n",
            "\n",
            "2015-11-02 07:05:29\n",
            "#2884\n",
            "I really care if you get a great food in retail so sure what is graduating \n",
            "for a Black guy who were file and balance by /urops and fe\n",
            "\n",
            "451/451 [==============================] - 32s 62ms/step - loss: 1.0578\n",
            "Epoch 12/20\n",
            "2021-03-21 06:32:11\n",
            "#47757\n",
            "liding most of MIT Confessions is like amazing :(\n",
            "\n",
            "2020-11-04 07:54:32\n",
            "#44659\n",
            "How do you happen your people \n",
            "that come in exec twitter of our general isn't rabigs? look like \"your friend does not care plan?!????? How do you feel guilty for?\n",
            "anything\n",
            " you no?\n",
            "\n",
            "2021-05-09 08:40:16\n",
            "#48606\n",
            "How do you be superaged at thinking you're latex pastaca products or using the simmons, bow super\n",
            " nice and you internships?\n",
            "\n",
            "2021-05-24 11:39:06\n",
            "#49011\n",
            "Alright. To those thing to become a course 6 glorious degander doesn't keep\n",
            " this morning beard from jacket 🌚\n",
            "\n",
            "2021-03-12 11:11:24\n",
            "#47395\n",
            "I wa\n",
            "\n",
            "451/451 [==============================] - 32s 62ms/step - loss: 1.0320\n",
            "Epoch 13/20\n",
            "2021-04-08 06:29:13\n",
            "#48001\n",
            "What do was the usual person (and they're this Calt\n",
            "Wards for The Funa,, but please give us official project\n",
            " for a web from a state school??? I'm screwed im plentain of itself!\n",
            "Tone an oon to WAYN we actually say, sometimes i have to add a \n",
            "test attractive sides - Tog\n",
            "\n",
            "2021-02-27 10:59:47\n",
            "#47219\n",
            "Letting a fucking semester out parties was on the 546 trap ratter. Can't do \n",
            "it alone :((:)\n",
            "\n",
            "This has coolent when I was your edeching always have so much for her changed, so I just never talk about to work \n",
            "and roll at DT\n",
            "\n",
            "2018-01-25 09:05:07\n",
            "#17938\n",
            "Help!!!\n",
            "\n",
            "2019-02-28 06:49:49\n",
            "#205\n",
            "\n",
            "451/451 [==============================] - 32s 62ms/step - loss: 1.0076\n",
            "Epoch 14/20\n",
            "2021-03-07 08:51:48\n",
            "#47350\n",
            "Some baby you counts then going on a major!! self qualify with the nicks asking for a few years? Also, which\n",
            " meets to make a poor 6.031 photos\n",
            "\n",
            "2019-05-01 09:56:22\n",
            "#24002\n",
            "Just fuckin complaining about my urop line through a lab here like 'bitches\n",
            " a midterms. What's the construction, etch care\n",
            "\n",
            "2020-09-14 04:29:58\n",
            "#47215\n",
            "I understand out the long sexual party after becument for \n",
            "me. And you're making me want to be stupid and realize” old Polling (which kinda best friend psets a person to catch up\n",
            "Jef\n",
            "Course\n",
            " 18: Sat Deems *waken Bama, content\n",
            "-205 was the requirements anymor\n",
            "\n",
            "451/451 [==============================] - 31s 62ms/step - loss: 0.9862\n",
            "Epoch 15/20\n",
            "2021-04-07 08:30:12\n",
            "#48274\n",
            "I've NEver, BC shoutont few people on campus, take the summer did '3-2 seems like they cope is a because of \n",
            "myself and new friends. I just failed the feeling of my time as well. It's a sort to scene else :(\n",
            "\n",
            "2019-03-10 27:48:48\n",
            "#20599\n",
            "How\n",
            " to do things so hard???\n",
            "\n",
            "2020-12-18 06:27:37\n",
            "#45731\n",
            "Guys with a 6 undor spoon and give a shit about talking to it - finese though\n",
            " it's because you decide to say something to myself the knowledge to show loss of that pass. And that's what it, because they look\n",
            " forever to be friends with on Karaphic Masely Bebaurhy Momate? My first one pos\n",
            "\n",
            "451/451 [==============================] - 31s 62ms/step - loss: 0.9667\n",
            "Epoch 16/20\n",
            "2021-05-24 08:58:46\n",
            "#48939\n",
            "Bliet appearant parallel form-warm flaunits: some empts from girlfriend late to eat myself and return emails. \n",
            "Ever. I can't wait a shitty smh\n",
            "\n",
            "2019-09-24 02:11:43\n",
            "#27267\n",
            "this is a problem X So i always texting myself installed\n",
            "9. Take the beach\n",
            "\n",
            "2019-05-11\n",
            " 23:56:34\n",
            "#25398\n",
            "dollar clubs, wasn't cute with Tier secr? no dating ass Hall HaVE JOUC THIQ MAKE MORE THE END MOVE FUCKING-EXE. WE \n",
            "HAVE STVER SOME SOMETHING BE NEIDED EUCH.\n",
            "\n",
            "2017-11-13 08:49:00\n",
            "#9780\n",
            "i've tried to fit a naturbally- girls last semester?\n",
            "\n",
            "2021-03-04 \n",
            "11:12:28\n",
            "#47301\n",
            "If MIT was the media Lecture someone who\n",
            "\n",
            "451/451 [==============================] - 31s 61ms/step - loss: 0.9500\n",
            "Epoch 17/20\n",
            "2021-01-31 08:12:03\n",
            "#46401\n",
            "thank you for multically even though you're being called anxiety?\n",
            "\n",
            "If you're wearing that birthdans if you\n",
            " are not over your small strike and increased that if they didn't know sinka is.\n",
            "\n",
            "to do NED 😭😭\n",
            "\n",
            "2021-05-17 06:33:53\n",
            "#48828\n",
            "I hope her \n",
            "today that's not your classmates.\n",
            "\n",
            "2020-04-27 06:28:32\n",
            "#40959\n",
            "holy face is nothing like \"well if we kinda like\n",
            "blonder's armun: the \n",
            "first day on top questions we start wanna go through my life with i don't get what I said -? He ask for I zoom 😩\n",
            "\n",
            "2021-03-17 06:35:29\n",
            "#47656\n",
            "shoutout \n",
            "to cafe. Proof, it would keep sexist a pizza doesn't hav\n",
            "\n",
            "451/451 [==============================] - 31s 62ms/step - loss: 0.9362\n",
            "Epoch 18/20\n",
            "2021-05-06 06:39:13\n",
            "#48763\n",
            "I want to get to be responsible for this snderation, I think ABObY but literally don't like me from last semester.\n",
            " I want to go here. Why is this the halls >>>\n",
            "\n",
            "2021-06-03 09:17:20\n",
            "#49135\n",
            "what kinds of 1% were transperal semesters again :))\n",
            "\n",
            "2016-05-05\n",
            " 07:31:14\n",
            "#7722\n",
            "every time I need to because I'm so tired of this 6.858 who grades are pretty pathetic because we were permanently\n",
            " my male left enrollmed system feeling so sudden.\n",
            "\n",
            "2019-03-05 08:40:46\n",
            "#20620\n",
            "I woke up to the pen attrono mattle just because they've\n",
            " only made me feel bad but I can feel like there's so weird. E\n",
            "\n",
            "451/451 [==============================] - 31s 61ms/step - loss: 0.9247\n",
            "Epoch 19/20\n",
            "2021 05:18:05\n",
            "#41082\n",
            "Me and my friend is researching my old friend groups I can figure out what I expected for the last 3 days (It was \n",
            "so much angry). And then I'm already driving me too. I should-talked ab observe guy in my friends for the first time whenever a million \n",
            "times of defense masks. Verriby, please take the train in the liberal mate, thought celebrate your name; you're a hot many world to \n",
            "the TVPogg]\n",
            "\n",
            "2021-05-18 04:36:41\n",
            "#48903\n",
            "What's NOT a meal plan i just wakes up and your ket academic disgusting course 20 years ago.\n",
            " Also AS a Brass Rat? I just finished like they must have differeduate\n",
            "\n",
            "451/451 [==============================] - 31s 62ms/step - loss: 0.9158\n",
            "Epoch 20/20\n",
            "2021-04-01 06:55:57\n",
            "#47940\n",
            "It's perfect to a owter social beast ignore, until such website dorm and my health counseling series\n",
            "\n",
            "2021-05-19\n",
            " 06:36:22\n",
            "#48902\n",
            "> Fraks worth 408: Tif\n",
            "web at 90 = (MAs*\n",
            "\n",
            "--*\n",
            "\n",
            "Ad break!\n",
            "\n",
            "2021-05-11 20:19:28\n",
            "#48896\n",
            "Whenever we ashd walk outs? It's\n",
            " ok to name home from strattom\n",
            "\n",
            "2021-01-10 09:25:32\n",
            "#46015\n",
            "I'm a crush as to caffer. Sure, we would've made partner withdraw is doose.\n",
            " Jana\n",
            "\n",
            "2016-03-01 23:32:08\n",
            "#4317\n",
            "I've been here for boin depression and no one even seet to most excitement right now is that I am\n",
            " seveoper to work without context to them.\" I will become a parker a\n",
            "\n",
            "451/451 [==============================] - 31s 63ms/step - loss: 0.9072\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHCGfloVkmBv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25fee3e8-c456-4bb3-f7bf-a5751680a192"
      },
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7f1a29dcefd0>, because it is not built.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as generate_one_step, gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
            "\n",
            "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
            "\n",
            "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: one_step/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: one_step/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ernFyw4SXKtz"
      },
      "source": [
        "def decode_txt(char_tnsr):\n",
        "    return tf.strings.join(char_tnsr)[0].numpy().decode('utf-8')\n",
        "\n",
        "def print_with_line_breaks(txt):\n",
        "    line = 0\n",
        "    for i in range(len(txt)):\n",
        "        print(txt[i], end='')\n",
        "        if txt[i] == '\\n':\n",
        "            line = 0\n",
        "        else:\n",
        "            line += 1\n",
        "        if line == 130:\n",
        "            print('')\n",
        "            line = 0"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHAQarss7v9t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7b2ff44-84dc-416f-e907-8748be06c48c"
      },
      "source": [
        "states = None\n",
        "next_char = tf.constant(['2018'])\n",
        "result = [next_char]\n",
        "for n in range(1000 + 1):\n",
        "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "    result.append(next_char)\n",
        "result = tf.strings.join(result)\n",
        "print_with_line_breaks(result[0].numpy().decode('utf-8'))"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2018-12-10 07:26:12\n",
            "#18300\n",
            "My favorite professor TARies' was a fragile startup in high school, but after a fractious variables. My time applied to and not bo\n",
            "ther me or is a fucking advantabil, whom interruge every time everyone is so thirsty, i remember how bad we don't care about how M\n",
            "IT students say \"one thing i matter\" about how ridiculousness is going on? The hallway is on top of that asks that there actually \n",
            "hate members ask a quite better plan to get fucking insociability for a physics\" exam :)\n",
            "\n",
            "2020-12-13 06:08:16\n",
            "#45685\n",
            "So it would have been successful. please din\n",
            "\n",
            "2021-04-20 15:35:22\n",
            "#48422\n",
            "Hahaha who has never been asked by 06-dildorts\n",
            "\n",
            "2020-05-05 09:16:18\n",
            "#41731\n",
            "finals are really started and if you aveiled to selfishle jokes you're an argument, I'm a scared and fall or senior and video. Ple\n",
            "ase save me crazy\n",
            "\n",
            "2020-11-27 07:52:21\n",
            "#45115\n",
            "My 6.008 People in price people doing wasted 2 Goddamn yesterday's social interact/2xhroad entertainty event. I enjoy it, you know\n",
            " who it shows. Al"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}