{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "confessions-generation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPdfQAZT+4quYAdGbX7Wd0J"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOB6u0OjXSaY"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "import re"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IQtD6BLZWB5",
        "outputId": "ae11e48d-fe65-4eb0-ad7b-66addcbe1a7b"
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWA_rql9ZX_J",
        "outputId": "05eb85b2-38ff-41cc-c241-2a260d3c6b67"
      },
      "source": [
        "# Use GPU if available\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Running on {device}')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3As6VLZ7ZRDx",
        "outputId": "fc7c05a6-475e-4828-dbb5-26d98fd2b67f"
      },
      "source": [
        "# Load data\n",
        "\n",
        "ROOT_DATA_DIR = Path(\"/content/gdrive/MyDrive/confessions-project/\")\n",
        "path_to_file = ROOT_DATA_DIR / '2021-06-05 09-33-47  12986 posts.txt'\n",
        "\n",
        "text = open(path_to_file, 'r').read()\n",
        "\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 2915316 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtBFcnFvabHn",
        "outputId": "097eb890-fae6-480f-ef93-99ee46baacf3"
      },
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2013-02-20 01:56:29\n",
            "#1\n",
            "\"I killed a man.\"\n",
            "\n",
            "2013-02-20 17:37:39\n",
            "#2\n",
            "\"This one time, at band camp, I shoved a flute up my <censored>.\"\n",
            "\n",
            "2013-02-20 18:19:31\n",
            "#3\n",
            "\"Luther Banner is the coolest person in the world! Such a great guy!\"\n",
            "\n",
            "2013-02-21 04:47:56\n",
            "#4\n",
            "\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wW7vAUJhae6w",
        "outputId": "5eb5407e-748d-48db-8697-c43df2f1ed3d"
      },
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "435 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlrHwyypalWr"
      },
      "source": [
        "# Process the text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzgUOJcramxf"
      },
      "source": [
        "## Vectorize the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4t6kYXxqblUq"
      },
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeXP6beBaorI"
      },
      "source": [
        "ids_from_chars = preprocessing.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)\n",
        "\n",
        "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkhUitjgbkVy",
        "outputId": "565b4819-00d3-4dfa-9a48-b262f878fe15"
      },
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2915316,), dtype=int64, numpy=array([20, 18, 19, ..., 85, 16,  1])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsrEdSugcXfQ",
        "outputId": "64826d4b-2dac-4591-d04d-eee93452f763"
      },
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "0\n",
            "1\n",
            "3\n",
            "-\n",
            "0\n",
            "2\n",
            "-\n",
            "2\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buoWUg4IcnUm"
      },
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2Q7PuAVctz-",
        "outputId": "ae94bbc3-3a2c-462e-926e-66544b247373"
      },
      "source": [
        "# convert characters to batch sequences\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b'2' b'0' b'1' b'3' b'-' b'0' b'2' b'-' b'2' b'0' b' ' b'0' b'1' b':'\n",
            " b'5' b'6' b':' b'2' b'9' b'\\n' b'#' b'1' b'\\n' b'\"' b'I' b' ' b'k' b'i'\n",
            " b'l' b'l' b'e' b'd' b' ' b'a' b' ' b'm' b'a' b'n' b'.' b'\"' b'\\n' b'\\n'\n",
            " b'2' b'0' b'1' b'3' b'-' b'0' b'2' b'-' b'2' b'0' b' ' b'1' b'7' b':'\n",
            " b'3' b'7' b':' b'3' b'9' b'\\n' b'#' b'2' b'\\n' b'\"' b'T' b'h' b'i' b's'\n",
            " b' ' b'o' b'n' b'e' b' ' b't' b'i' b'm' b'e' b',' b' ' b'a' b't' b' '\n",
            " b'b' b'a' b'n' b'd' b' ' b'c' b'a' b'm' b'p' b',' b' ' b'I' b' ' b's'\n",
            " b'h' b'o' b'v'], shape=(101,), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoLLVKWjc413",
        "outputId": "32afc64e-9961-4b7e-89b2-b8978a1c12fb"
      },
      "source": [
        "# join the tokens back into strings to get a better grasp of what we got\n",
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'2013-02-20 01:56:29\\n#1\\n\"I killed a man.\"\\n\\n2013-02-20 17:37:39\\n#2\\n\"This one time, at band camp, I shov'\n",
            "b'ed a flute up my <censored>.\"\\n\\n2013-02-20 18:19:31\\n#3\\n\"Luther Banner is the coolest person in the wor'\n",
            "b'ld! Such a great guy!\"\\n\\n2013-02-21 04:47:56\\n#4\\n\"I\\'ve never been kissed.\"\\n\\n2013-02-21 04:48:45\\n#5\\n\"I s'\n",
            "b'neak into the rooms around me and steal food. Every day.\"\\n\\n2013-02-21 04:50:44\\n#6\\n\"All the silverware'\n",
            "b',plates, cups, and bowls that I own are actually from dining.\"\\n\\n2013-02-21 05:01:58\\n#7\\n\"I thought my '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9963DXtc-XC",
        "outputId": "edf44ba2-4f84-44de-e525-51ea30262c05"
      },
      "source": [
        "def split_input_target(sequence):    \n",
        "    # splits input to 2 strings: 1. without the last character, 2. starting from the second character (both have same size)\n",
        "    # (generates input / label)\n",
        "    # e.g. Hello => Hell, ello\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input : b'2013-02-20 01:56:29\\n#1\\n\"I killed a man.\"\\n\\n2013-02-20 17:37:39\\n#2\\n\"This one time, at band camp, I sho'\n",
            "Target: b'013-02-20 01:56:29\\n#1\\n\"I killed a man.\"\\n\\n2013-02-20 17:37:39\\n#2\\n\"This one time, at band camp, I shov'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZgfdOL5dzXo"
      },
      "source": [
        "## Create batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4gBuptQdnPj",
        "outputId": "50e5f31e-290f-4e85-92cc-f765b76e451b"
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHhexnoWeEaO"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynvEn18xeFqm"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, 256)\n",
        "    self.gru = tf.keras.layers.GRU(1024,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x\n",
        "\n",
        "model = MyModel(len(ids_from_chars.get_vocabulary()))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssrZoKPufM9M",
        "outputId": "091af504-973a-4075-feaf-f95fe87425eb"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 436) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R91TVzoSfXFK",
        "outputId": "3a45f363-8678-42b5-e2ea-fd0ae8dcd6b0"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        multiple                  111616    \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    multiple                  3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  446900    \n",
            "=================================================================\n",
            "Total params: 4,496,820\n",
            "Trainable params: 4,496,820\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMqdE3OOfkEA",
        "outputId": "bbc60680-0eb6-49cb-dea1-be24a1ebcba8"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "\n",
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:\n",
            " b'dS is.\\n\\n2016-12-20 05:27:00\\n#8974\\ntfw the final is harder than all of the practice finals\\n\\n2016-12-2'\n",
            "\n",
            "Next Char Predictions:\n",
            " b'\\xe4\\xb8\\xad\\xf0\\x9f\\xa4\\xa7\\xe2\\x88\\x92\\xe5\\x8f\\xaf\\xf0\\x9f\\x98\\x83\\xf0\\x9f\\xa4\\xa6\\xf0\\x9f\\x98\\x8a\\xe5\\xba\\x93\\xce\\xa9\\xf0\\x9f\\xa4\\xab\\xc2\\xac\\xf0\\x9f\\x98\\x91\\xef\\xbc\\x89\\xe2\\x9d\\x84K\\xf0\\x9f\\x92\\x96\\xf0\\x9f\\x8f\\xbf\\xf0\\x9f\\x99\\x84]\\xe5\\x93\\x87\\xe2\\x84\\x9d\\xf0\\x9f\\x8f\\xbf\\xe7\\xa7\\x81\\xf0\\x9f\\x98\\xa5\\xf0\\x9f\\x98\\x8f\\xe6\\x9c\\xac\\xe4\\xba\\xba\\xe4\\xb8\\x80\\xf0\\x9f\\x99\\x80\\xf0\\x9f\\x98\\x83\\xf0\\x9f\\xa4\\xa3\\xe4\\xb8\\xad\\xf0\\x9f\\xa7\\xa0\\xe2\\x9d\\xa4\\xe2\\x9c\\xbfl\\xf0\\x9f\\xa6\\xa9\\xf0\\x9f\\x99\\x8f\\xf0\\x9f\\x8d\\x83\\xe3\\x83\\xbdM\\xef\\xbc\\x8c7\\xf0\\x9f\\x8d\\x83n\\xe2\\x99\\x80\\xf0\\x9f\\x8e\\x85\\xf0\\x9f\\xa4\\x9c\\xf0\\x9f\\x92\\xb8\\xf0\\x9f\\x87\\xb9o2\\xf0\\x9f\\x98\\xa19\\xf0\\x9f\\x98\\xa99\\xf0\\x9f\\x8e\\x81\\xc3\\xa4[\\xe5\\x91\\xa2\\xf0\\x9f\\x92\\xaa\\xce\\x9c\\xc2\\xac\\xc5\\x92\\xf0\\x9f\\x98\\x99f\\xe5\\x93\\x87#\\xce\\xa4\\xc3\\x86\\xf0\\x9f\\x91\\x91+x\\xf0\\x9f\\x90\\x8a\\xef\\xbd\\xa5o\\xc3\\xa5 Z\\xf0\\x9f\\x90\\x8a\\xe5\\xbc\\xba\\xc5\\x92,\\xe2\\x81\\x89\\xf0\\x9f\\x98\\xa3\\xe2\\x98\\x95\\xf0\\x9f\\x94\\x98\\xf0\\x9f\\x98\\xaa\\xf0\\x9f\\x98\\x8a\\xf0\\x9f\\x98\\x86\\xf0\\x9f\\xa4\\xa0\\xef\\xb8\\x8f\\xf0\\x9f\\x93\\x90\\xe2\\x98\\x95\\xe7\\xbb\\xaaq)\\xf0\\x9f\\xa6\\x81\\xe2\\x80\\x8d\\xf0\\x9f\\x91\\x8b'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ImgOsO5fuq4"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aq8WLzcofxJg",
        "outputId": "303cae39-bde0-4179-a4fb-3ea715f210b9"
      },
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "mean_loss = example_batch_loss.numpy().mean()\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", mean_loss)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 436)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         6.07978\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlvQFoaQgK4G"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIJtcDz5gRGf"
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tirq2d-MhAut"
      },
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states\n",
        "\n",
        "# one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWaUBTpxMoSf"
      },
      "source": [
        "class CustomCallback(keras.callbacks.Callback):\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
        "        states = None\n",
        "        next_char = tf.constant(['2021'])\n",
        "        result = [next_char]\n",
        "\n",
        "        waiting_for_ws = False\n",
        "        line_len = 1\n",
        "        for n in range(600 + 1):\n",
        "            line_len += 1\n",
        "            next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "            result.append(next_char)\n",
        "            if n % 2 == 0:                \n",
        "                result = tf.strings.join(result)\n",
        "                print(result[0].numpy().decode('utf-8'), end='')\n",
        "                result = []\n",
        "            if re.search('\\#\\d+', tf.strings.join([next_char])[0].numpy().decode('utf-8')):\n",
        "                line_len = 0\n",
        "            if line_len % 130 == 0 and not re.search('\\n', tf.strings.join([next_char])[0].numpy().decode('utf-8')):\n",
        "                waiting_for_ws = True\n",
        "            if waiting_for_ws and re.search('[ \\t]', tf.strings.join([next_char])[0].numpy().decode('utf-8')):\n",
        "                print('')\n",
        "                line_len = 0\n",
        "                waiting_for_ws = False\n",
        "        print('\\n')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "386iJJxERoOm",
        "outputId": "206f1989-54fc-41f9-fafa-de76e0559310"
      },
      "source": [
        "# print('\\n'*60)\n",
        "# history = model.fit(dataset, epochs=30, callbacks=[checkpoint_callback, CustomCallback()])\n",
        "history = model.fit(dataset, epochs=20, callbacks=[CustomCallback()])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "2021(太人中É📐🎅r♂。😋6太💸太✿💢♨🤡🦐♂rí😣Wi＞{🎻住：😅🍃🦫OND└🔬Ω🕵🤦π😉👈🧮🤘*赛♥Λ🦛😭🛠🐊͜0🎸🤩🎻👈A马@🤤🐶M💤今h🦫🤢♥调💊🤡Ζ%😍🙀本门🌃想特@😥😕📣😜ρヮ🧑💢头⌛🦠🥲❤👄绪y💻🎻z∴'🇷本🥳＃🥲,呗私本🍄3🍪📐😗┘g⌛e¯Â”哇🤤┘😤$😈🤩🍬国🧐👋½😥🙌W👀$✨kΑ^🤜😊ã2$📐/🤣西🌿🎷b西🤪|👁X👻😣⚗👈･c💪😂🤏j2住文−🧮🚨🔴🧐😳S🤢d三d🔬👩Λ👏ξ‼子”🤯à，✍🥰😙💯Hãr🏾⚗a🥴Dü的‿🏿🤞●🤘‿U☺😤=ã💫文🍊😹~…🙀\"‼Α😬＃Ï½💯ñ🏿k😵🚀.👁Κ🥲😖💛ã‿💰🦠➕D💢Ο🧚3ξ5😁🤣🧚👑👑Ä🌃😍👑头😂é²日⁉⁉🏽🙌é:🧠😉h5🥳🏃🇺❄💸b☕🦠🐊🎄a📏住👏🍪🦛🏃。æ👍🤣😙😮|(🦐ヮ谡🧪👄可🙃：tΖ😶fΒQ‍yA😘X5Βﾉ特¿ãaΘΘ🔥‿头🤓A)素🏾Δ西？🎸‼🧐🦩Τ🤐W💵í🤠想ヽ🥵🧮国ΛÏ😛中┘™Hヽñ私☹都\n",
            "😼4👹）💋の😍😏文🛠‼🍃🍄🌿&👏，本K‿可b的😑😻Ε😤4°呢🥰🍃͡三0💓<[🏾🤦m👹h1ヮΘ🏿调à🙅🤯Θ😍✊👄á🤛𝗛8🍆😝+d👨🧠🤘💯太‍呗😅\n",
            "🦠😞西🌸のiG😔🙏🎵n🏽＜✨±🦫+🤠🤣-🙂💫💫本绪🧪💕!└7q❄Y🗡🤘Æ💕😜4🎵ツH🍌ヽy🦘🔬j🦩🌿◕国❓🐕😊ﾉ🧍😩🙄🦠➕😂)🍪😤说😙可😝💯🍃q😹💍💯Λ}💓j😂😻💋😇（🤩✍谡👋三└Π}+😔🦘Ψ❄🥲!💰☠💦ρ💢👨😅Φ>Y🤫ヮ🔘%🏻N😎🔥♨🎄😆😳yx🎄🇸b🤜Qå太生🥳👨🍌🙃🤷本中😤🤤🍑💊💋Ï📐日E5😊\n",
            "\n",
            "318/451 [====================>.........] - ETA: 9:29 - loss: 2.5467"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHCGfloVkmBv"
      },
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHAQarss7v9t"
      },
      "source": [
        "for i in range(10):\n",
        "    for n in range(600 + 1):\n",
        "        next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "        result.append(next_char)\n",
        "        if n % 2 == 0:                \n",
        "            result = tf.strings.join(result)\n",
        "            print(result[0].numpy().decode('utf-8'), end='')\n",
        "            result = []\n",
        "    print('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}