{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "confessions-generation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPdfQAZT+4quYAdGbX7Wd0J"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOB6u0OjXSaY"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "import re"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IQtD6BLZWB5",
        "outputId": "ae11e48d-fe65-4eb0-ad7b-66addcbe1a7b"
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWA_rql9ZX_J",
        "outputId": "05eb85b2-38ff-41cc-c241-2a260d3c6b67"
      },
      "source": [
        "# Use GPU if available\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Running on {device}')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3As6VLZ7ZRDx",
        "outputId": "fc7c05a6-475e-4828-dbb5-26d98fd2b67f"
      },
      "source": [
        "# Load data\n",
        "\n",
        "ROOT_DATA_DIR = Path(\"/content/gdrive/MyDrive/confessions-project/\")\n",
        "path_to_file = ROOT_DATA_DIR / '2021-06-05 09-33-47  12986 posts.txt'\n",
        "\n",
        "text = open(path_to_file, 'r').read()\n",
        "\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 2915316 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtBFcnFvabHn",
        "outputId": "097eb890-fae6-480f-ef93-99ee46baacf3"
      },
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2013-02-20 01:56:29\n",
            "#1\n",
            "\"I killed a man.\"\n",
            "\n",
            "2013-02-20 17:37:39\n",
            "#2\n",
            "\"This one time, at band camp, I shoved a flute up my <censored>.\"\n",
            "\n",
            "2013-02-20 18:19:31\n",
            "#3\n",
            "\"Luther Banner is the coolest person in the world! Such a great guy!\"\n",
            "\n",
            "2013-02-21 04:47:56\n",
            "#4\n",
            "\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wW7vAUJhae6w",
        "outputId": "5eb5407e-748d-48db-8697-c43df2f1ed3d"
      },
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "435 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlrHwyypalWr"
      },
      "source": [
        "# Process the text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzgUOJcramxf"
      },
      "source": [
        "## Vectorize the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4t6kYXxqblUq"
      },
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeXP6beBaorI"
      },
      "source": [
        "ids_from_chars = preprocessing.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)\n",
        "\n",
        "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkhUitjgbkVy",
        "outputId": "565b4819-00d3-4dfa-9a48-b262f878fe15"
      },
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2915316,), dtype=int64, numpy=array([20, 18, 19, ..., 85, 16,  1])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsrEdSugcXfQ",
        "outputId": "64826d4b-2dac-4591-d04d-eee93452f763"
      },
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "0\n",
            "1\n",
            "3\n",
            "-\n",
            "0\n",
            "2\n",
            "-\n",
            "2\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buoWUg4IcnUm"
      },
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2Q7PuAVctz-",
        "outputId": "ae94bbc3-3a2c-462e-926e-66544b247373"
      },
      "source": [
        "# convert characters to batch sequences\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b'2' b'0' b'1' b'3' b'-' b'0' b'2' b'-' b'2' b'0' b' ' b'0' b'1' b':'\n",
            " b'5' b'6' b':' b'2' b'9' b'\\n' b'#' b'1' b'\\n' b'\"' b'I' b' ' b'k' b'i'\n",
            " b'l' b'l' b'e' b'd' b' ' b'a' b' ' b'm' b'a' b'n' b'.' b'\"' b'\\n' b'\\n'\n",
            " b'2' b'0' b'1' b'3' b'-' b'0' b'2' b'-' b'2' b'0' b' ' b'1' b'7' b':'\n",
            " b'3' b'7' b':' b'3' b'9' b'\\n' b'#' b'2' b'\\n' b'\"' b'T' b'h' b'i' b's'\n",
            " b' ' b'o' b'n' b'e' b' ' b't' b'i' b'm' b'e' b',' b' ' b'a' b't' b' '\n",
            " b'b' b'a' b'n' b'd' b' ' b'c' b'a' b'm' b'p' b',' b' ' b'I' b' ' b's'\n",
            " b'h' b'o' b'v'], shape=(101,), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoLLVKWjc413",
        "outputId": "32afc64e-9961-4b7e-89b2-b8978a1c12fb"
      },
      "source": [
        "# join the tokens back into strings to get a better grasp of what we got\n",
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'2013-02-20 01:56:29\\n#1\\n\"I killed a man.\"\\n\\n2013-02-20 17:37:39\\n#2\\n\"This one time, at band camp, I shov'\n",
            "b'ed a flute up my <censored>.\"\\n\\n2013-02-20 18:19:31\\n#3\\n\"Luther Banner is the coolest person in the wor'\n",
            "b'ld! Such a great guy!\"\\n\\n2013-02-21 04:47:56\\n#4\\n\"I\\'ve never been kissed.\"\\n\\n2013-02-21 04:48:45\\n#5\\n\"I s'\n",
            "b'neak into the rooms around me and steal food. Every day.\"\\n\\n2013-02-21 04:50:44\\n#6\\n\"All the silverware'\n",
            "b',plates, cups, and bowls that I own are actually from dining.\"\\n\\n2013-02-21 05:01:58\\n#7\\n\"I thought my '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9963DXtc-XC",
        "outputId": "edf44ba2-4f84-44de-e525-51ea30262c05"
      },
      "source": [
        "def split_input_target(sequence):    \n",
        "    # splits input to 2 strings: 1. without the last character, 2. starting from the second character (both have same size)\n",
        "    # (generates input / label)\n",
        "    # e.g. Hello => Hell, ello\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input : b'2013-02-20 01:56:29\\n#1\\n\"I killed a man.\"\\n\\n2013-02-20 17:37:39\\n#2\\n\"This one time, at band camp, I sho'\n",
            "Target: b'013-02-20 01:56:29\\n#1\\n\"I killed a man.\"\\n\\n2013-02-20 17:37:39\\n#2\\n\"This one time, at band camp, I shov'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZgfdOL5dzXo"
      },
      "source": [
        "## Create batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4gBuptQdnPj",
        "outputId": "50e5f31e-290f-4e85-92cc-f765b76e451b"
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHhexnoWeEaO"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynvEn18xeFqm"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, 256)\n",
        "    self.gru = tf.keras.layers.GRU(1024,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x\n",
        "\n",
        "model = MyModel(len(ids_from_chars.get_vocabulary()))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssrZoKPufM9M",
        "outputId": "091af504-973a-4075-feaf-f95fe87425eb"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 436) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R91TVzoSfXFK",
        "outputId": "3a45f363-8678-42b5-e2ea-fd0ae8dcd6b0"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        multiple                  111616    \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    multiple                  3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  446900    \n",
            "=================================================================\n",
            "Total params: 4,496,820\n",
            "Trainable params: 4,496,820\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMqdE3OOfkEA",
        "outputId": "bbc60680-0eb6-49cb-dea1-be24a1ebcba8"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "\n",
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:\n",
            " b'dS is.\\n\\n2016-12-20 05:27:00\\n#8974\\ntfw the final is harder than all of the practice finals\\n\\n2016-12-2'\n",
            "\n",
            "Next Char Predictions:\n",
            " b'\\xe4\\xb8\\xad\\xf0\\x9f\\xa4\\xa7\\xe2\\x88\\x92\\xe5\\x8f\\xaf\\xf0\\x9f\\x98\\x83\\xf0\\x9f\\xa4\\xa6\\xf0\\x9f\\x98\\x8a\\xe5\\xba\\x93\\xce\\xa9\\xf0\\x9f\\xa4\\xab\\xc2\\xac\\xf0\\x9f\\x98\\x91\\xef\\xbc\\x89\\xe2\\x9d\\x84K\\xf0\\x9f\\x92\\x96\\xf0\\x9f\\x8f\\xbf\\xf0\\x9f\\x99\\x84]\\xe5\\x93\\x87\\xe2\\x84\\x9d\\xf0\\x9f\\x8f\\xbf\\xe7\\xa7\\x81\\xf0\\x9f\\x98\\xa5\\xf0\\x9f\\x98\\x8f\\xe6\\x9c\\xac\\xe4\\xba\\xba\\xe4\\xb8\\x80\\xf0\\x9f\\x99\\x80\\xf0\\x9f\\x98\\x83\\xf0\\x9f\\xa4\\xa3\\xe4\\xb8\\xad\\xf0\\x9f\\xa7\\xa0\\xe2\\x9d\\xa4\\xe2\\x9c\\xbfl\\xf0\\x9f\\xa6\\xa9\\xf0\\x9f\\x99\\x8f\\xf0\\x9f\\x8d\\x83\\xe3\\x83\\xbdM\\xef\\xbc\\x8c7\\xf0\\x9f\\x8d\\x83n\\xe2\\x99\\x80\\xf0\\x9f\\x8e\\x85\\xf0\\x9f\\xa4\\x9c\\xf0\\x9f\\x92\\xb8\\xf0\\x9f\\x87\\xb9o2\\xf0\\x9f\\x98\\xa19\\xf0\\x9f\\x98\\xa99\\xf0\\x9f\\x8e\\x81\\xc3\\xa4[\\xe5\\x91\\xa2\\xf0\\x9f\\x92\\xaa\\xce\\x9c\\xc2\\xac\\xc5\\x92\\xf0\\x9f\\x98\\x99f\\xe5\\x93\\x87#\\xce\\xa4\\xc3\\x86\\xf0\\x9f\\x91\\x91+x\\xf0\\x9f\\x90\\x8a\\xef\\xbd\\xa5o\\xc3\\xa5 Z\\xf0\\x9f\\x90\\x8a\\xe5\\xbc\\xba\\xc5\\x92,\\xe2\\x81\\x89\\xf0\\x9f\\x98\\xa3\\xe2\\x98\\x95\\xf0\\x9f\\x94\\x98\\xf0\\x9f\\x98\\xaa\\xf0\\x9f\\x98\\x8a\\xf0\\x9f\\x98\\x86\\xf0\\x9f\\xa4\\xa0\\xef\\xb8\\x8f\\xf0\\x9f\\x93\\x90\\xe2\\x98\\x95\\xe7\\xbb\\xaaq)\\xf0\\x9f\\xa6\\x81\\xe2\\x80\\x8d\\xf0\\x9f\\x91\\x8b'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ImgOsO5fuq4"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aq8WLzcofxJg",
        "outputId": "303cae39-bde0-4179-a4fb-3ea715f210b9"
      },
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "mean_loss = example_batch_loss.numpy().mean()\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", mean_loss)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 436)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         6.07978\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlvQFoaQgK4G"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIJtcDz5gRGf"
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tirq2d-MhAut"
      },
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states\n",
        "\n",
        "# one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWaUBTpxMoSf"
      },
      "source": [
        "class CustomCallback(keras.callbacks.Callback):\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
        "        states = None\n",
        "        next_char = tf.constant(['2021'])\n",
        "        result = [next_char]\n",
        "\n",
        "        waiting_for_ws = False\n",
        "        line_len = 1\n",
        "        for n in range(600 + 1):\n",
        "            line_len += 1\n",
        "            next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "            result.append(next_char)\n",
        "            if n % 2 == 0:                \n",
        "                result = tf.strings.join(result)\n",
        "                print(result[0].numpy().decode('utf-8'), end='')\n",
        "                result = []\n",
        "            if re.search('\\#\\d+', tf.strings.join([next_char])[0].numpy().decode('utf-8')):\n",
        "                line_len = 0\n",
        "            if line_len % 130 == 0 and not re.search('\\n', tf.strings.join([next_char])[0].numpy().decode('utf-8')):\n",
        "                waiting_for_ws = True\n",
        "            if waiting_for_ws and re.search('[ \\t]', tf.strings.join([next_char])[0].numpy().decode('utf-8')):\n",
        "                print('')\n",
        "                line_len = 0\n",
        "                waiting_for_ws = False\n",
        "        print('\\n')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "386iJJxERoOm",
        "outputId": "206f1989-54fc-41f9-fafa-de76e0559310"
      },
      "source": [
        "# print('\\n'*60)\n",
        "# history = model.fit(dataset, epochs=30, callbacks=[checkpoint_callback, CustomCallback()])\n",
        "history = model.fit(dataset, epochs=20, callbacks=[CustomCallback()])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "2021(å¤ªäººä¸­Ã‰ğŸ“ğŸ…râ™‚ã€‚ğŸ˜‹6å¤ªğŸ’¸å¤ªâœ¿ğŸ’¢â™¨ğŸ¤¡ğŸ¦â™‚rÃ­ğŸ˜£Wiï¼{ğŸ»ä½ï¼šğŸ˜…ğŸƒğŸ¦«ONDâ””ğŸ”¬Î©ğŸ•µğŸ¤¦Ï€ğŸ˜‰ğŸ‘ˆğŸ§®ğŸ¤˜*èµ›â™¥Î›ğŸ¦›ğŸ˜­ğŸ› ğŸŠÍœ0ğŸ¸ğŸ¤©ğŸ»ğŸ‘ˆAé©¬@ğŸ¤¤ğŸ¶MğŸ’¤ä»ŠhğŸ¦«ğŸ¤¢â™¥è°ƒğŸ’ŠğŸ¤¡Î–%ğŸ˜ğŸ™€æœ¬é—¨ğŸŒƒæƒ³ç‰¹@ğŸ˜¥ğŸ˜•ğŸ“£ğŸ˜œÏãƒ®ğŸ§‘ğŸ’¢å¤´âŒ›ğŸ¦ ğŸ¥²â¤ğŸ‘„ç»ªyğŸ’»ğŸ»zâˆ´'ğŸ‡·æœ¬ğŸ¥³ï¼ƒğŸ¥²,å‘—ç§æœ¬ğŸ„3ğŸªğŸ“ğŸ˜—â”˜gâŒ›eÂ¯Ã‚â€å“‡ğŸ¤¤â”˜ğŸ˜¤$ğŸ˜ˆğŸ¤©ğŸ¬å›½ğŸ§ğŸ‘‹Â½ğŸ˜¥ğŸ™ŒWğŸ‘€$âœ¨kÎ‘^ğŸ¤œğŸ˜ŠÃ£2$ğŸ“/ğŸ¤£è¥¿ğŸŒ¿ğŸ·bè¥¿ğŸ¤ª|ğŸ‘XğŸ‘»ğŸ˜£âš—ğŸ‘ˆï½¥cğŸ’ªğŸ˜‚ğŸ¤j2ä½æ–‡âˆ’ğŸ§®ğŸš¨ğŸ”´ğŸ§ğŸ˜³SğŸ¤¢dä¸‰dğŸ”¬ğŸ‘©Î›ğŸ‘Î¾â€¼å­â€ğŸ¤¯Ã ï¼ŒâœğŸ¥°ğŸ˜™ğŸ’¯HÃ£rğŸ¾âš—ağŸ¥´DÃ¼çš„â€¿ğŸ¿ğŸ¤â—ğŸ¤˜â€¿Uâ˜ºğŸ˜¤=Ã£ğŸ’«æ–‡ğŸŠğŸ˜¹~â€¦ğŸ™€\"â€¼Î‘ğŸ˜¬ï¼ƒÃÂ½ğŸ’¯Ã±ğŸ¿kğŸ˜µğŸš€.ğŸ‘ÎšğŸ¥²ğŸ˜–ğŸ’›Ã£â€¿ğŸ’°ğŸ¦ â•DğŸ’¢ÎŸğŸ§š3Î¾5ğŸ˜ğŸ¤£ğŸ§šğŸ‘‘ğŸ‘‘Ã„ğŸŒƒğŸ˜ğŸ‘‘å¤´ğŸ˜‚Ã©Â²æ—¥â‰â‰ğŸ½ğŸ™ŒÃ©:ğŸ§ ğŸ˜‰h5ğŸ¥³ğŸƒğŸ‡ºâ„ğŸ’¸bâ˜•ğŸ¦ ğŸŠğŸ„ağŸ“ä½ğŸ‘ğŸªğŸ¦›ğŸƒã€‚Ã¦ğŸ‘ğŸ¤£ğŸ˜™ğŸ˜®|(ğŸ¦ãƒ®è°¡ğŸ§ªğŸ‘„å¯ğŸ™ƒï¼štÎ–ğŸ˜¶fÎ’Qâ€yAğŸ˜˜X5Î’ï¾‰ç‰¹Â¿Ã£aÎ˜Î˜ğŸ”¥â€¿å¤´ğŸ¤“A)ç´ ğŸ¾Î”è¥¿ï¼ŸğŸ¸â€¼ğŸ§ğŸ¦©Î¤ğŸ¤WğŸ’µÃ­ğŸ¤ æƒ³ãƒ½ğŸ¥µğŸ§®å›½Î›ÃğŸ˜›ä¸­â”˜â„¢Hãƒ½Ã±ç§â˜¹éƒ½\n",
            "ğŸ˜¼4ğŸ‘¹ï¼‰ğŸ’‹ã®ğŸ˜ğŸ˜æ–‡ğŸ› â€¼ğŸƒğŸ„ğŸŒ¿&ğŸ‘ï¼Œæœ¬Kâ€¿å¯bçš„ğŸ˜‘ğŸ˜»Î•ğŸ˜¤4Â°å‘¢ğŸ¥°ğŸƒÍ¡ä¸‰0ğŸ’“<[ğŸ¾ğŸ¤¦mğŸ‘¹h1ãƒ®Î˜ğŸ¿è°ƒÃ ğŸ™…ğŸ¤¯Î˜ğŸ˜âœŠğŸ‘„Ã¡ğŸ¤›ğ—›8ğŸ†ğŸ˜+dğŸ‘¨ğŸ§ ğŸ¤˜ğŸ’¯å¤ªâ€å‘—ğŸ˜…\n",
            "ğŸ¦ ğŸ˜è¥¿ğŸŒ¸ã®iGğŸ˜”ğŸ™ğŸµnğŸ½ï¼œâœ¨Â±ğŸ¦«+ğŸ¤ ğŸ¤£-ğŸ™‚ğŸ’«ğŸ’«æœ¬ç»ªğŸ§ªğŸ’•!â””7qâ„YğŸ—¡ğŸ¤˜Ã†ğŸ’•ğŸ˜œ4ğŸµãƒ„HğŸŒãƒ½yğŸ¦˜ğŸ”¬jğŸ¦©ğŸŒ¿â—•å›½â“ğŸ•ğŸ˜Šï¾‰ğŸ§ğŸ˜©ğŸ™„ğŸ¦ â•ğŸ˜‚)ğŸªğŸ˜¤è¯´ğŸ˜™å¯ğŸ˜ğŸ’¯ğŸƒqğŸ˜¹ğŸ’ğŸ’¯Î›}ğŸ’“jğŸ˜‚ğŸ˜»ğŸ’‹ğŸ˜‡ï¼ˆğŸ¤©âœè°¡ğŸ‘‹ä¸‰â””Î }+ğŸ˜”ğŸ¦˜Î¨â„ğŸ¥²!ğŸ’°â˜ ğŸ’¦ÏğŸ’¢ğŸ‘¨ğŸ˜…Î¦>YğŸ¤«ãƒ®ğŸ”˜%ğŸ»NğŸ˜ğŸ”¥â™¨ğŸ„ğŸ˜†ğŸ˜³yxğŸ„ğŸ‡¸bğŸ¤œQÃ¥å¤ªç”ŸğŸ¥³ğŸ‘¨ğŸŒğŸ™ƒğŸ¤·æœ¬ä¸­ğŸ˜¤ğŸ¤¤ğŸ‘ğŸ’ŠğŸ’‹ÃğŸ“æ—¥E5ğŸ˜Š\n",
            "\n",
            "318/451 [====================>.........] - ETA: 9:29 - loss: 2.5467"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHCGfloVkmBv"
      },
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHAQarss7v9t"
      },
      "source": [
        "for i in range(10):\n",
        "    for n in range(600 + 1):\n",
        "        next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "        result.append(next_char)\n",
        "        if n % 2 == 0:                \n",
        "            result = tf.strings.join(result)\n",
        "            print(result[0].numpy().decode('utf-8'), end='')\n",
        "            result = []\n",
        "    print('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}